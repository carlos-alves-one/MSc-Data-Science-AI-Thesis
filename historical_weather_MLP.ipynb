{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-MSc-Data-Science-AI-Thesis/blob/main/historical_weather_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jwu9i0Ihryi"
      },
      "source": [
        "### Goldsmiths University of London\n",
        "### MSc. Data Science and Artificial Intelligence\n",
        "### Module: Final Project in Data Science (2023-2024)\n",
        "### Author: Carlos Manuel De Oliveira Alves\n",
        "### Student: cdeol003"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:** This script performs data analysis and MLP on historical weather data for solar energy forecasting."
      ],
      "metadata": {
        "id": "lhqRRL_IWRck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This script imports the time module, records the script's start time, and prints a formatted timestamp indicating when the script began execution."
      ],
      "metadata": {
        "id": "_o3PxoPpWVv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "start_time_str = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"Script started at: {start_time_str}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvso6ofBWYx9",
        "outputId": "2e74b0b5-73bb-464a-ae32-3b0362a2c245"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script started at: 2024-08-23 22:03:25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evcLSlczN8zt"
      },
      "source": [
        "# Data Collection and Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdj8e-22ODYt"
      },
      "source": [
        "## 1. Load the Data\n",
        "   - Connect to Google Drive to access the dataset\n",
        "   - Load the data from the provided CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "gJuT3ZuRhd5-",
        "outputId": "d984acb6-7369-4fc8-ae36-226d986e8cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          0                    1  \\\n",
              "datetime                2021-09-01 00:00:00  2021-09-01 00:00:00   \n",
              "temperature                            14.2                 13.9   \n",
              "dewpoint                               11.6                 11.5   \n",
              "rain                                    0.0                  0.0   \n",
              "snowfall                                0.0                  0.0   \n",
              "surface_pressure                     1015.9               1010.7   \n",
              "cloudcover_total                         31                   33   \n",
              "cloudcover_low                           31                   37   \n",
              "cloudcover_mid                            0                    0   \n",
              "cloudcover_high                          11                    0   \n",
              "windspeed_10m                      7.083333             5.111111   \n",
              "winddirection_10m                         8                  359   \n",
              "shortwave_radiation                     0.0                  0.0   \n",
              "direct_solar_radiation                  0.0                  0.0   \n",
              "diffuse_radiation                       0.0                  0.0   \n",
              "latitude                               57.6                 57.6   \n",
              "longitude                              21.7                 22.2   \n",
              "data_block_id                           1.0                  1.0   \n",
              "\n",
              "                                          2                    3  \\\n",
              "datetime                2021-09-01 00:00:00  2021-09-01 00:00:00   \n",
              "temperature                            14.0                 14.6   \n",
              "dewpoint                               12.5                 11.5   \n",
              "rain                                    0.0                  0.0   \n",
              "snowfall                                0.0                  0.0   \n",
              "surface_pressure                     1015.0               1017.3   \n",
              "cloudcover_total                         31                    0   \n",
              "cloudcover_low                           34                    0   \n",
              "cloudcover_mid                            0                    0   \n",
              "cloudcover_high                           0                    0   \n",
              "windspeed_10m                      6.333333             8.083333   \n",
              "winddirection_10m                       355                  297   \n",
              "shortwave_radiation                     0.0                358.0   \n",
              "direct_solar_radiation                  0.0                277.0   \n",
              "diffuse_radiation                       0.0                 81.0   \n",
              "latitude                               57.6                 57.6   \n",
              "longitude                              22.7                 23.2   \n",
              "data_block_id                           1.0                  1.0   \n",
              "\n",
              "                                          4  \n",
              "datetime                2021-09-01 00:00:00  \n",
              "temperature                            15.7  \n",
              "dewpoint                               12.9  \n",
              "rain                                    0.0  \n",
              "snowfall                                0.0  \n",
              "surface_pressure                     1014.0  \n",
              "cloudcover_total                         22  \n",
              "cloudcover_low                           25  \n",
              "cloudcover_mid                            0  \n",
              "cloudcover_high                           0  \n",
              "windspeed_10m                      8.416667  \n",
              "winddirection_10m                         5  \n",
              "shortwave_radiation                     0.0  \n",
              "direct_solar_radiation                  0.0  \n",
              "diffuse_radiation                       0.0  \n",
              "latitude                               57.6  \n",
              "longitude                              23.7  \n",
              "data_block_id                           1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-274a9480-858b-42b3-be9a-76f8dce5c309\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>datetime</th>\n",
              "      <td>2021-09-01 00:00:00</td>\n",
              "      <td>2021-09-01 00:00:00</td>\n",
              "      <td>2021-09-01 00:00:00</td>\n",
              "      <td>2021-09-01 00:00:00</td>\n",
              "      <td>2021-09-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>temperature</th>\n",
              "      <td>14.2</td>\n",
              "      <td>13.9</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.6</td>\n",
              "      <td>15.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dewpoint</th>\n",
              "      <td>11.6</td>\n",
              "      <td>11.5</td>\n",
              "      <td>12.5</td>\n",
              "      <td>11.5</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rain</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snowfall</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_pressure</th>\n",
              "      <td>1015.9</td>\n",
              "      <td>1010.7</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>1017.3</td>\n",
              "      <td>1014.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cloudcover_total</th>\n",
              "      <td>31</td>\n",
              "      <td>33</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cloudcover_low</th>\n",
              "      <td>31</td>\n",
              "      <td>37</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cloudcover_mid</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cloudcover_high</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>windspeed_10m</th>\n",
              "      <td>7.083333</td>\n",
              "      <td>5.111111</td>\n",
              "      <td>6.333333</td>\n",
              "      <td>8.083333</td>\n",
              "      <td>8.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>winddirection_10m</th>\n",
              "      <td>8</td>\n",
              "      <td>359</td>\n",
              "      <td>355</td>\n",
              "      <td>297</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shortwave_radiation</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>358.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>direct_solar_radiation</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diffuse_radiation</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>latitude</th>\n",
              "      <td>57.6</td>\n",
              "      <td>57.6</td>\n",
              "      <td>57.6</td>\n",
              "      <td>57.6</td>\n",
              "      <td>57.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>longitude</th>\n",
              "      <td>21.7</td>\n",
              "      <td>22.2</td>\n",
              "      <td>22.7</td>\n",
              "      <td>23.2</td>\n",
              "      <td>23.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_block_id</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-274a9480-858b-42b3-be9a-76f8dce5c309')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-274a9480-858b-42b3-be9a-76f8dce5c309 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-274a9480-858b-42b3-be9a-76f8dce5c309');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d707012d-349d-4ca0-8533-40b558f12866\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d707012d-349d-4ca0-8533-40b558f12866')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d707012d-349d-4ca0-8533-40b558f12866 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()\n",
        "\n",
        "# Import the pandas library and give it the alias 'pd' for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset Amazon Review Details from Google Drive\n",
        "data_path = '/content/drive/MyDrive/big_data_project/historical_weather.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "data.head(5).T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dE9A6ovkoLX"
      },
      "source": [
        "> The dataset contains various weather variables that will be useful for your project on short-term solar energy forecasting. Here are the columns present in the dataset:\n",
        "\n",
        "1. datetime: Timestamp of the data.\n",
        "2. temperature: Temperature in degrees Celsius.\n",
        "3. dewpoint: Dewpoint in degrees Celsius.\n",
        "4. rain: Rainfall amount in mm.\n",
        "5. snowfall: Snowfall amount in mm.\n",
        "6. surface_pressure: Surface pressure in hPa.\n",
        "7. cloudcover_total: Total cloud cover percentage.\n",
        "8. cloudcover_low: Low-level cloud cover percentage.\n",
        "9. cloudcover_mid: Mid-level cloud cover percentage.\n",
        "10. cloudcover_high: High-level cloud cover percentage.\n",
        "11. windspeed_10m: Wind speed at 10 meters in m/s.\n",
        "12. winddirection_10m: Wind direction at 10 meters in degrees.\n",
        "13. shortwave_radiation: Shortwave radiation in W/m².\n",
        "14. direct_solar_radiation: Direct solar radiation in W/m².\n",
        "15. diffuse_radiation: Diffuse radiation in W/m².\n",
        "16. latitude: Latitude coordinate.\n",
        "17. longitude: Longitude coordinate.\n",
        "18. data_block_id: Data block identifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b6lX9aM4KJ2"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf2-TvC84LeX",
        "outputId": "a0e116ab-fa73-4822-e562-6df5f28656f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-> Number of rows.....: 1,710,802\n",
            "-> Number of columns..: 18\n",
            "\n",
            "-> Summary of the dataset:\n",
            "\n",
            "                            count         mean         std    min     25%  \\\n",
            "temperature             1710802.0     5.740968    8.025647  -23.7     0.0   \n",
            "dewpoint                1710802.0     2.240312    7.224357  -25.9    -2.6   \n",
            "rain                    1710802.0     0.049620    0.207911    0.0     0.0   \n",
            "snowfall                1710802.0     0.016049    0.074629    0.0     0.0   \n",
            "surface_pressure        1710802.0  1009.281515   13.088915  942.9  1001.5   \n",
            "cloudcover_total        1710802.0    60.912696   37.769048    0.0    25.0   \n",
            "cloudcover_low          1710802.0    46.685927   40.747598    0.0     3.0   \n",
            "cloudcover_mid          1710802.0    34.406980   38.327693    0.0     0.0   \n",
            "cloudcover_high         1710802.0    36.051408   41.358521    0.0     0.0   \n",
            "windspeed_10m           1710802.0     4.849871    2.475450    0.0     3.0   \n",
            "winddirection_10m       1710802.0   197.869419   89.937978    0.0   139.0   \n",
            "shortwave_radiation     1710802.0   106.490504  179.944912    0.0     0.0   \n",
            "direct_solar_radiation  1710802.0    64.452917  133.409951    0.0     0.0   \n",
            "diffuse_radiation       1710802.0    42.037587   61.952251    0.0     0.0   \n",
            "latitude                1710802.0    58.649999    0.687387   57.6    57.9   \n",
            "longitude               1710802.0    24.949999    2.015564   21.7    23.2   \n",
            "data_block_id           1710802.0   319.270778  183.729798    1.0   160.0   \n",
            "\n",
            "                           50%          75%      max  \n",
            "temperature                5.1    11.200000    32.60  \n",
            "dewpoint                   1.7     7.200000    23.80  \n",
            "rain                       0.0     0.000000    16.80  \n",
            "snowfall                   0.0     0.000000     2.66  \n",
            "surface_pressure        1010.4  1018.000000  1049.30  \n",
            "cloudcover_total          72.0   100.000000   100.00  \n",
            "cloudcover_low            39.0    94.000000   100.00  \n",
            "cloudcover_mid            16.0    72.000000   100.00  \n",
            "cloudcover_high           10.0    85.000000   100.00  \n",
            "windspeed_10m              4.5     6.277778    21.75  \n",
            "winddirection_10m        208.0   263.000000   360.00  \n",
            "shortwave_radiation        1.0   140.000000   849.00  \n",
            "direct_solar_radiation     0.0    47.000000   754.00  \n",
            "diffuse_radiation          1.0    74.000000   388.00  \n",
            "latitude                  58.5    59.100000    59.70  \n",
            "longitude                 24.7    26.700000    28.20  \n",
            "data_block_id            319.0   478.000000   637.00  \n",
            "\n",
            "-> Data types and missing values:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1710802 entries, 0 to 1710801\n",
            "Data columns (total 18 columns):\n",
            " #   Column                  Dtype  \n",
            "---  ------                  -----  \n",
            " 0   datetime                object \n",
            " 1   temperature             float64\n",
            " 2   dewpoint                float64\n",
            " 3   rain                    float64\n",
            " 4   snowfall                float64\n",
            " 5   surface_pressure        float64\n",
            " 6   cloudcover_total        int64  \n",
            " 7   cloudcover_low          int64  \n",
            " 8   cloudcover_mid          int64  \n",
            " 9   cloudcover_high         int64  \n",
            " 10  windspeed_10m           float64\n",
            " 11  winddirection_10m       int64  \n",
            " 12  shortwave_radiation     float64\n",
            " 13  direct_solar_radiation  float64\n",
            " 14  diffuse_radiation       float64\n",
            " 15  latitude                float64\n",
            " 16  longitude               float64\n",
            " 17  data_block_id           float64\n",
            "dtypes: float64(12), int64(5), object(1)\n",
            "memory usage: 234.9+ MB\n",
            "None\n",
            "\n",
            "-> Missing values in each column:\n",
            "\n",
            "datetime                  0\n",
            "temperature               0\n",
            "dewpoint                  0\n",
            "rain                      0\n",
            "snowfall                  0\n",
            "surface_pressure          0\n",
            "cloudcover_total          0\n",
            "cloudcover_low            0\n",
            "cloudcover_mid            0\n",
            "cloudcover_high           0\n",
            "windspeed_10m             0\n",
            "winddirection_10m         0\n",
            "shortwave_radiation       0\n",
            "direct_solar_radiation    0\n",
            "diffuse_radiation         0\n",
            "latitude                  0\n",
            "longitude                 0\n",
            "data_block_id             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Print the number of rows and columns\n",
        "print(\"\\n-> Number of rows.....: {:,}\".format(data.shape[0]))\n",
        "print(\"-> Number of columns..:\", data.shape[1])\n",
        "\n",
        "# Display a summary of the dataset\n",
        "print(\"\\n-> Summary of the dataset:\\n\")\n",
        "print(data.describe().T)\n",
        "\n",
        "# Check data types and missing values\n",
        "print(\"\\n-> Data types and missing values:\\n\")\n",
        "print(data.info())\n",
        "\n",
        "# Checking for missing values\n",
        "print(\"\\n-> Missing values in each column:\\n\")\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Key findings from the dataset analysis:\n",
        "\n",
        "1. Dataset Size:\n",
        "   - Number of rows: 1,710,802\n",
        "   - Number of columns: 18\n",
        "\n",
        "2. Data Types:\n",
        "   - 12 columns are of type float64\n",
        "   - 5 columns are of type int64\n",
        "   - 1 column (datetime) is of type object\n",
        "\n",
        "3. Missing Values:\n",
        "   - There are no missing values in any of the columns\n",
        "\n",
        "4. Key Statistics:\n",
        "   a. Temperature:\n",
        "      - Mean: 5.74°C\n",
        "      - Minimum: -23.7°C\n",
        "      - Maximum: 32.6°C\n",
        "\n",
        "   b. Precipitation:\n",
        "      - Mean rainfall: 0.05 mm\n",
        "      - Maximum rainfall: 16.8 mm\n",
        "      - Mean snowfall: 0.016 mm\n",
        "      - Maximum snowfall: 2.66 mm\n",
        "\n",
        "   c. Pressure:\n",
        "      - Mean surface pressure: 1009.28 hPa\n",
        "      - Range: 942.9 hPa to 1049.3 hPa\n",
        "\n",
        "   d. Cloud Cover:\n",
        "      - Mean total cloud cover: 60.91%\n",
        "      - All cloud cover types (total, low, mid, high) range from 0% to 100%\n",
        "\n",
        "   e. Wind:\n",
        "      - Mean wind speed at 10m: 4.85 m/s\n",
        "      - Maximum wind speed at 10m: 21.75 m/s\n",
        "      - Wind direction ranges from 0° to 360°\n",
        "\n",
        "   f. Solar Radiation:\n",
        "      - Mean shortwave radiation: 106.49 W/m²\n",
        "      - Maximum shortwave radiation: 849 W/m²\n",
        "\n",
        "   g. Location:\n",
        "      - Latitude range: 57.6° to 59.7°\n",
        "      - Longitude range: 21.7° to 28.2°\n",
        "\n",
        "5. Additional Observations:\n",
        "   Given the latitude and longitude ranges, the dataset appears to cover a specific geographic area, possibly in northern Europe.\n",
        "   - A wide range of weather conditions is represented, from very cold (-23.7°C) to warm (32.6°C) temperatures.\n",
        "   - The data includes various meteorological parameters, making it suitable for comprehensive weather analysis.\n",
        "   - The 'data_block_id' column suggests the data might be organized into different blocks or periods.\n",
        "\n",
        "This dataset provides a rich source of meteorological information, covering temperature, precipitation, atmospheric pressure, cloud cover, wind, and solar radiation. It is a comprehensive weather dataset for the specified geographic area, with no missing values, which is excellent for analysis."
      ],
      "metadata": {
        "id": "R9Y7aOra5DNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This comprehensive meteorological dataset contains 1,710,802 rows and 18 columns with no missing values. It covers a specific geographic area, likely in northern Europe (latitude 57.6° to 59.7°, longitude 21.7° to 28.2°), and includes a wide range of weather parameters. The data shows diverse conditions, with temperatures ranging from -23.7°C to 32.6°C (mean 5.74°C), precipitation levels up to 16.8mm for rain and 2.66mm for snow, and surface pressure between 942.9 and 1049.3 hPa. Cloud cover, wind speed (up to 21.75 m/s at 10m height), wind direction, and solar radiation measurements are also included. The dataset's completeness and variety of meteorological factors make it an excellent resource for in-depth weather analysis and modelling for the region.\n"
      ],
      "metadata": {
        "id": "smQJ1z6q5m0C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJKRMuKZAyZ6"
      },
      "source": [
        "##Visualize Feature Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrEnZsEXBD97"
      },
      "outputs": [],
      "source": [
        "# Importing visualization libraries and numeric operations for data analysis and plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Determine the number of rows and columns for the subplot grid\n",
        "num_plots = len(numeric_columns)\n",
        "num_columns = 4\n",
        "num_rows = math.ceil(num_plots / num_columns)\n",
        "\n",
        "# Create a grid of histograms\n",
        "plt.figure(figsize=(20, num_rows * 4))\n",
        "for i, column in enumerate(numeric_columns, 1):\n",
        "    plt.subplot(num_rows, num_columns, i)\n",
        "    sns.histplot(data[column], kde=True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Key findings from the weather dataset visualizations:\n",
        "\n",
        "1. Data Distribution:\n",
        "   - Shows that the data is evenly distributed across different data_block_ids, with each block containing around 14,000 entries. This suggests a consistent data collection process over time or across different locations.\n",
        "\n",
        "2. Weather Parameters:\n",
        "   a. Temperature and Dewpoint:\n",
        "      - Both temperature and dewpoint follow a roughly normal distribution.\n",
        "      - Temperature ranges from about -20°C to 30°C, with a peak around 5-10°C.\n",
        "      - Dewpoint has a similar range but peaks slightly lower, around 0-5°C.\n",
        "\n",
        "   b. Precipitation:\n",
        "      - Both rain and snowfall show extremely right-skewed distributions.\n",
        "      - Most observations have little to no precipitation, with rare heavy rain or snow occurrences.\n",
        "\n",
        "   c. Surface Pressure:\n",
        "      - Follows a normal distribution, centered around 1010-1015 hPa.\n",
        "\n",
        "   d. Cloud Cover:\n",
        "      - Total, low, and mid-level cloud cover all show U-shaped distributions.\n",
        "      - There are peaks at 0% (clear skies) and 100% (overcast), with fewer observations.\n",
        "\n",
        "   e. Wind:\n",
        "      - Wind speed follows a right-skewed distribution, with most speeds below ten m/s and a peak around 4-5 m/s.\n",
        "      - Wind direction shows peaks at cardinal and intercardinal directions, suggesting possible measurement bias or prevalent wind patterns.\n",
        "\n",
        "   f. Solar Radiation:\n",
        "      - Shortwave, direct solar, and diffuse radiation all show right-skewed distributions, peaking at 0.\n",
        "      - This is expected due to the day-night cycle and varying cloud cover.\n",
        "\n",
        "3. Geographical Distribution:\n",
        "   - Latitude and longitude plots show distinct peaks, indicating that data was collected from specific locations rather than continuously across a region.\n",
        "\n",
        "4. Data Quality:\n",
        "   - The consistent counts across data blocks and the absence of unexpected gaps or outliers in most plots suggest good overall data quality.\n",
        "   - The wind direction plot shows some potential measurement biases that need further investigation.\n",
        "\n",
        "5. Climate Insights:\n",
        "   - The temperature and precipitation patterns suggest a temperate climate with occasional snow, consistent with the latitude range (around 57.5°-59.5°N).\n",
        "   - The pressure distribution and cloud cover patterns indicate a location with variable weather conditions, possibly influenced by maritime factors.\n",
        "\n",
        "These visualizations provide a comprehensive overview of the weather patterns in the dataset, highlighting the distributions and relationships between various meteorological parameters. The data is high quality and suitable for detailed climate analysis or weather modelling for the specific region it represents."
      ],
      "metadata": {
        "id": "LvYDE_gG8G8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The weather dataset visualizations reveal a comprehensive and consistently collected set of meteorological data, likely from a temperate climate region. The data is evenly distributed across data blocks, containing about 14,000 entries. Temperature and dewpoint follow normal distributions centred around 5-10°C and 0-5°C, respectively, while precipitation shows rare heavy rain or snow occurrences. Surface pressure is usually distributed around 1010-1015 hPa. Cloud cover exhibits U-shaped distributions, indicating frequent clear or overcast conditions. Wind speeds are typically below ten m/s, peaking at 4-5 m/s, with directional data showing potential measurement biases. Solar radiation data reflects expected day-night cycles and cloud cover variations. The latitude (57.5°-59.5°N) and longitude plots suggest data collection from specific locations rather than a continuous region. Overall, the visualizations indicate high-quality data suitable for detailed climate analysis, representing a temperate zone with variable weather conditions, possibly influenced by maritime factors."
      ],
      "metadata": {
        "id": "z4E06MmM9BiE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUEfqLdYmTZq"
      },
      "source": [
        "##Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bipzRh47mYtK"
      },
      "outputs": [],
      "source": [
        "# Select only columns with numeric data types for the correlation matrix\n",
        "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Display a correlation matrix\n",
        "correlation_matrix = numeric_data.corr()\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Key findings from the correlation matrix:\n",
        "\n",
        "1. Temperature and Dewpoint:\n",
        "   - Robust positive correlation (0.93), indicating that as the temperature rises, dewpoint typically rises.\n",
        "\n",
        "2. Cloud Cover:\n",
        "   - Strong positive correlations among different cloud cover types (total, low, mid, high), with the strongest between total and low cloud cover (0.84).\n",
        "   - Negative correlation with temperature, especially for low cloud cover (-0.35).\n",
        "\n",
        "3. Solar Radiation:\n",
        "   - Positive correlations among shortwave, direct solar, and diffuse radiation (0.83-0.97).\n",
        "   - Moderate negative correlation with temperature (-0.40 to -0.45), suggesting cooler temperatures during higher solar radiation periods (possibly due to seasonal effects).\n",
        "\n",
        "4. Precipitation:\n",
        "   - Weak positive correlation between rain and cloud cover (0.15-0.27).\n",
        "   - Snowfall shows a weak negative correlation with temperature (-0.19) and a weak positive with cloud cover (0.21-0.32).\n",
        "\n",
        "5. Surface Pressure:\n",
        "   - Weak to moderate negative correlations with cloud cover (-0.28 to -0.34) and precipitation (-0.17 to -0.26).\n",
        "\n",
        "6. Wind:\n",
        "   - Weak correlations overall, with the strongest being a negative correlation between wind speed and longitude (-0.32).\n",
        "\n",
        "7. Geographical Factors:\n",
        "   - Latitude and longitude show very weak correlations with most variables, suggesting minimal impact of location within the study area on weather patterns.\n",
        "\n",
        "8. Data Block ID:\n",
        "   - Very weak correlations with all variables indicate consistent data collection across blocks.\n",
        "\n",
        "Overall, the matrix reveals expected weather relationships (e.g., temperature-dewpoint, cloud cover-radiation) while showing minimal geographical influences within the study area. The data appears consistent across collection periods, supporting its reliability for further analysis."
      ],
      "metadata": {
        "id": "DKSc5y6M90Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The correlation matrix reveals several critical relationships in the weather dataset. Temperature and dewpoint show a robust positive correlation (0.93). At the same time, different types of cloud cover (total, low, mid, high) are strongly interconnected, with total and low cloud cover having the highest correlation (0.84). Solar radiation components (shortwave, direct, diffuse) are highly correlated (0.83-0.97) and show a moderate negative relationship with temperature (-0.40 to -0.45), possibly due to seasonal effects. Precipitation variables have weak to moderate correlations with cloud cover and temperature, while surface pressure shows weak negative correlations with cloud cover and precipitation. Wind parameters and geographical factors (latitude, longitude) generally exhibit weak correlations with other variables, suggesting minimal impact of location within the study area on weather patterns. The consistent weak correlations of the data block ID with all variables indicate reliable data collection across different periods. Overall, the matrix confirms expected meteorological relationships while highlighting the dataset's consistency and suitability for comprehensive weather analysis."
      ],
      "metadata": {
        "id": "lHeSUnILAu-6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A2mzEr5mzPl"
      },
      "source": [
        "##Features Correlation Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DDdcOkOm0XA"
      },
      "outputs": [],
      "source": [
        "# Select the variables of interest based on the important correlations\n",
        "selected_vars = [\n",
        "    'temperature', 'dewpoint', 'cloudcover_low', 'cloudcover_total',\n",
        "    'direct_solar_radiation', 'diffuse_radiation', 'shortwave_radiation',\n",
        "    'surface_pressure', 'cloudcover_mid', 'windspeed_10m', 'winddirection_10m'\n",
        "]\n",
        "\n",
        "# Sample the data for the selected variables\n",
        "sampled_data = numeric_data[selected_vars].sample(frac=0.1, random_state=1)\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(sampled_data)\n",
        "plt.suptitle('Pair Plot of Important Weather Variable Correlations\\n', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Key findings from the scatter plot matrix:\n",
        "\n",
        "1. Temperature and Dewpoint: A strong positive linear relationship confirms their high correlation.\n",
        "\n",
        "2. Precipitation (Rain and Snowfall): Both show a concentration of data points near zero, with occasional higher values, indicating infrequent precipitation events.\n",
        "\n",
        "3. Cloud Cover (Total, Low, Mid, High): These variables show complex, non-linear relationships with each other and other parameters. There are noticeable clusters at 0% and 100% cover.\n",
        "\n",
        "4. Solar Radiation (Shortwave, Direct, Diffuse): These show strong positive relationships with each other and a negative relationship with cloud cover.\n",
        "\n",
        "5. Surface Pressure: It shows weak to moderate relationships with other variables, with some interesting patterns visible.\n",
        "\n",
        "6. Wind Speed has weak relationships with most variables but shows some patterns with pressure and radiation.\n",
        "\n",
        "7. Wind Direction: Shows a reasonably uniform distribution across all angles (0-360°), with some preference for specific directions.\n",
        "\n",
        "8. Latitude and Longitude: Show distinct clusters, suggesting data collection from specific locations rather than a continuous area.\n",
        "\n",
        "9. Data Block ID: Shows little relationship with other variables, indicating consistent data collection across different periods or locations.\n",
        "\n",
        "10. Complex Interactions: Many plots show complex, non-linear relationships between variables, highlighting the intricate nature of weather systems.\n",
        "\n",
        "This visualization reinforces the correlations seen in the previous matrix while providing additional insights into the distributions and complex relationships between weather parameters. It underscores the dataset's richness and potential for in-depth meteorological analysis."
      ],
      "metadata": {
        "id": "Bd4ftlMLBXPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The scatter plot matrix of weather parameters reveals complex relationships and distributions within the dataset. Temperature and dewpoint show a strong positive linear relationship, while precipitation variables are concentrated near zero with occasional higher values. Cloud cover variables exhibit non-linear relationships and clustering at 0% and 100%. Solar radiation components are strongly interrelated and negatively associated with cloud cover. Surface pressure and wind speed display weak to moderate relationships with other variables, with interesting patterns. Wind direction is relatively uniformly distributed with slight directional preferences. Latitude and longitude show distinct clusters, indicating data collection from specific locations. The data block ID's lack of solid relationships with other variables suggests consistent data collection across different periods or locations. Overall, this visualization reinforces the correlations observed in the previous matrix while highlighting the intricate, often non-linear interactions between weather parameters, underscoring the dataset's richness and potential for comprehensive meteorological analysis."
      ],
      "metadata": {
        "id": "RE3EXU19DAKV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt97QU1zrDUC"
      },
      "source": [
        "# Machine Learning with Spark/MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbQ9EKiNraHD"
      },
      "source": [
        "## Setup and Data *Loading*\n",
        "\n",
        "> First, we set up the Spark session and loaded the weather data into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt0itanordYL"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh6p-Of5sHae"
      },
      "source": [
        "> The script code uses PySpark to set up a Spark session, load a historical weather dataset from a CSV file into a Spark DataFrame, and display the first few rows of the DataFrame to verify the successful loading of the data, which is typically the initial step in a data processing pipeline using PySpark before performing further operations such as data cleaning, transformation, and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c8I0o46sJ58"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Import col function from PySpark SQL functions module\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Solar Energy Forecasting\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = data_path\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert to Pandas DataFrame and transpose\n",
        "pandas_df = df.limit(4).toPandas()\n",
        "transposed_df = pandas_df.T\n",
        "\n",
        "# Show the first few rows of the DataFrame to confirm successful load\n",
        "print(transposed_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJVdp-OSPgjw"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "> Since no missing values are per the EDA, we will focus on feature scaling and encoding any categorical variables if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX7qQdchPnM9"
      },
      "source": [
        "> The script code uses PySpark's machine learning library, specifically the `VectorAssembler` and `StandardScaler`, to preprocess the data by assembling the numerical features into a single vector column and then scaling those features to have zero mean and unit variance, which is a common technique in machine learning to ensure that all features have similar magnitudes and prevent certain features from dominating the model training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6QC0ynFPkyh"
      },
      "outputs": [],
      "source": [
        "# Import VectorAssembler and StandardScaler from PySpark ML feature module\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# scale numerical features 'datetime' and 'data_block_id' by encoded or extracted for features\n",
        "numeric_features = [col_name for col_name in df.columns if col_name not in ['datetime', 'data_block_id']]\n",
        "assembler = VectorAssembler(inputCols=numeric_features, outputCol=\"features_unscaled\")\n",
        "df_unscaled = assembler.transform(df)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withStd=True)\n",
        "scaler_model = scaler.fit(df_unscaled)\n",
        "df_scaled = scaler_model.transform(df_unscaled)\n",
        "\n",
        "# Show transformed data\n",
        "df_scaled.select(\"Features\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ktwvV6QsSa"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "> Create additional features that may help improve model performance, such as time-related features from the datetime column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K12cHFQ1Q7dr"
      },
      "source": [
        "> The script code further preprocesses the data by extracting relevant date and time information from the \"datetime\" column using PySpark's built-in functions `month()`, `dayofmonth()`, and `hour()`, creating new columns for the month, day, and hour. These new features are then added to the list of feature columns, and the `VectorAssembler` is re-run to include these additional features in the final assembled feature vector, resulting in a DataFrame (`df_final`) that now contains the original numerical features along with the newly extracted date and time features, all combined into a single \"features\" column ready for use in machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0wYa7mRQxQi"
      },
      "outputs": [],
      "source": [
        "# Import month, dayofmonth, and hour functions from PySpark SQL functions module\n",
        "from pyspark.sql.functions import month, dayofmonth, hour\n",
        "\n",
        "# Extracting date parts\n",
        "df = df.withColumn(\"month\", month(\"datetime\"))\n",
        "df = df.withColumn(\"day\", dayofmonth(\"datetime\"))\n",
        "df = df.withColumn(\"hour\", hour(\"datetime\"))\n",
        "\n",
        "# Re-run the assembler with the new features\n",
        "new_feature_cols = numeric_features + [\"month\", \"day\", \"hour\"]\n",
        "assembler = VectorAssembler(inputCols=new_feature_cols, outputCol=\"features\")\n",
        "df_final = assembler.transform(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFMwJmRaSPnR"
      },
      "source": [
        "## Building Machine Learning Models\n",
        "\n",
        "> Utilize Spark MLlib to train a model, such as linear regression, to predict solar radiation based on the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1roJxc7qfOtL"
      },
      "source": [
        "### MLP (Multilayer Perceptron)\n",
        "\n",
        "Multilayer Perceptrons (MLPs), also known as feedforward neural networks, are valuable for energy forecasting projects due to their versatility and ability to handle complex data relationships. MLPs excel at capturing non-linear interactions in energy systems where multiple factors interplay intricately. They offer flexibility in handling various input variables and can be used for regression and classification tasks in energy forecasting. MLPs automatically learn relevant features from input data, potentially uncovering hidden patterns. They are scalable and capable of processing large datasets and high-dimensional input spaces, making them suitable for complex energy forecasting scenarios. MLPs adapt to evolving consumption patterns and can be configured for multi-output prediction, allowing simultaneous forecasting of multiple energy variables. With proper preprocessing, they can handle datasets with missing values shared in real-world energy data. MLPs can be effectively combined with other models in ensemble methods to improve overall forecasting accuracy. They balance model complexity and interpretability compared to more advanced deep learning architectures. The architecture of MLPs is easily customizable to suit specific energy forecasting tasks. While implementing MLPs requires careful consideration of data preprocessing, architecture design, and hyperparameter tuning, they provide a powerful tool for capturing complex patterns in energy data, bridging the gap between simple statistical models and more advanced deep learning approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This script implements a Multilayer Perceptron (MLP) neural network using PySpark and TensorFlow to predict shortwave radiation based on historical weather data. It loads and preprocesses the data using PySpark, normalizes it, creates time series sequences, builds and trains an MLP model, generates predictions, and then post-processes the results. The script joins the predictions with the original data, performs data quality checks, and evaluates the model's performance using various regression metrics (RMSE, MAE, R2, MAPE). It leverages PySpark's distributed computing capabilities for data handling and TensorFlow for deep learning. It combines big data processing with neural network techniques to create a scalable solution for time series prediction on weather data. The script includes error handling and informative print statements to track progress and catch issues, concluding with a display of the final results, including the original data and the model's predictions."
      ],
      "metadata": {
        "id": "anvJ8VX5iJEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, isnan, when, abs as spark_abs, avg\n",
        "from pyspark.sql.types import DoubleType, FloatType\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"MLP with PySpark\").getOrCreate()\n",
        "\n",
        "def print_df_info(df, name):\n",
        "    print(f\"\\n--- {name} Info ---\")\n",
        "    print(f\"Count: {df.count()}\")\n",
        "    print(\"Schema:\")\n",
        "    df.printSchema()\n",
        "    print(\"Sample data:\")\n",
        "    df.show(5, truncate=False)\n",
        "\n",
        "try:\n",
        "    # Step 1: Load and prepare data\n",
        "    print(\"\\nStep 1: Loading and preparing data...\")\n",
        "    df = spark.read.csv(\"/content/drive/MyDrive/big_data_project/historical_weather.csv\", header=True, inferSchema=True)\n",
        "    pandas_df = df.select(col('shortwave_radiation')).toPandas()\n",
        "    data = pandas_df.values\n",
        "\n",
        "    # Step 2: Normalize the data\n",
        "    print(\"\\nStep 2: Normalizing data...\")\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Step 3: Prepare input/output sequences\n",
        "    print(\"\\nStep 3: Preparing input/output sequences...\")\n",
        "    X, y = [], []\n",
        "    sequence_length = 10  # Number of time steps to look back\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        X.append(scaled_data[i:i+sequence_length])\n",
        "        y.append(scaled_data[i+sequence_length])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Step 4: Build MLP model\n",
        "    print(\"\\nStep 4: Building MLP model...\")\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(sequence_length, 1)),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "    # Step 5: Train the model\n",
        "    print(\"\\nStep 5: Training the model...\")\n",
        "    model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Step 6: Make predictions\n",
        "    print(\"\\nStep 6: Making predictions...\")\n",
        "    predictions = model.predict(X)\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Predictions sample: {predictions[:5]}\")\n",
        "\n",
        "    # Step 7: Convert predictions back to original scale\n",
        "    print(\"\\nStep 7: Converting predictions to original scale...\")\n",
        "    # Reshape predictions to 2D array\n",
        "    predictions_2d = predictions.reshape(-1, 1)\n",
        "    predictions_original = scaler.inverse_transform(predictions_2d)\n",
        "    print(f\"Inverse transformed predictions shape: {predictions_original.shape}\")\n",
        "    print(f\"Inverse transformed predictions sample: {predictions_original[:5]}\")\n",
        "\n",
        "    # Step 8: Convert predictions to Spark DataFrame\n",
        "    print(\"\\nStep 8: Converting predictions to Spark DataFrame...\")\n",
        "    predictions_df = spark.createDataFrame(predictions_original.tolist(), [\"prediction\"])\n",
        "    print_df_info(predictions_df, \"Predictions DataFrame\")\n",
        "\n",
        "    # Step 9: Join with original data\n",
        "    print(\"\\nStep 9: Joining predictions with original data...\")\n",
        "    print_df_info(df, \"Original DataFrame\")\n",
        "\n",
        "    # Adjust the number of rows in predictions_df to match the original df\n",
        "    predictions_df = predictions_df.limit(df.count() - sequence_length)\n",
        "\n",
        "    result_df = df.withColumn(\"row_id\", monotonically_increasing_id()).join(\n",
        "        predictions_df.withColumn(\"row_id\", monotonically_increasing_id()),\n",
        "        \"row_id\"\n",
        "    ).drop(\"row_id\")\n",
        "    print_df_info(result_df, \"Result DataFrame\")\n",
        "\n",
        "    # Check for NaN or null values\n",
        "    print(\"\\nChecking for NaN or null values...\")\n",
        "    for column in result_df.columns:\n",
        "        null_count = result_df.filter(col(column).isNull()).count()\n",
        "        if result_df.schema[column].dataType in [DoubleType(), FloatType()]:\n",
        "            nan_count = result_df.filter(isnan(col(column))).count()\n",
        "            print(f\"Column '{column}': Null count = {null_count}, NaN count = {nan_count}\")\n",
        "        else:\n",
        "            print(f\"Column '{column}': Null count = {null_count}\")\n",
        "\n",
        "    # Step 10: Evaluate the model\n",
        "    print(\"\\nStep 10: Evaluating the model...\")\n",
        "    evaluator = RegressionEvaluator(labelCol=\"shortwave_radiation\", predictionCol=\"prediction\")\n",
        "\n",
        "    # Ensure 'shortwave_radiation' and 'prediction' columns exist and are of the correct type\n",
        "    if \"shortwave_radiation\" not in result_df.columns or \"prediction\" not in result_df.columns:\n",
        "        raise ValueError(\"'shortwave_radiation' or 'prediction' column missing from result_df\")\n",
        "\n",
        "    result_df = result_df.withColumn(\"shortwave_radiation\", col(\"shortwave_radiation\").cast(\"double\"))\n",
        "    result_df = result_df.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
        "\n",
        "    # Remove rows with NaN or null values in relevant columns\n",
        "    result_df = result_df.filter(\n",
        "        ~isnan(col(\"shortwave_radiation\")) & ~isnan(col(\"prediction\")) &\n",
        "        col(\"shortwave_radiation\").isNotNull() & col(\"prediction\").isNotNull()\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = [\"rmse\", \"mae\", \"r2\"]\n",
        "    for metric in metrics:\n",
        "        value = evaluator.evaluate(result_df, {evaluator.metricName: metric})\n",
        "        print(f\"{metric.upper()}: {value}\")\n",
        "\n",
        "    # Calculate MAPE\n",
        "    result_df = result_df.withColumn(\"mape\",\n",
        "        when(col(\"shortwave_radiation\") != 0,\n",
        "             spark_abs(col(\"shortwave_radiation\") - col(\"prediction\")) / col(\"shortwave_radiation\") * 100)\n",
        "        .otherwise(None)\n",
        "    ).withColumn(\"mape\", col(\"mape\").cast(\"double\"))  # Ensure MAPE is cast to double\n",
        "\n",
        "    # Calculate average MAPE using Spark SQL functions\n",
        "    mape = result_df.select(avg(\"mape\")).collect()[0][0]\n",
        "    print(f\"MAPE: {mape}%\")\n",
        "\n",
        "    # Step 11: Show the results\n",
        "    print(\"\\nStep 11: Showing final results...\")\n",
        "    result_df.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "finally:\n",
        "    # Stop the Spark session\n",
        "    print(\"\\nStopping Spark session...\")\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "id": "v-hz7UnLuiFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Evaluation\n",
        "\n",
        "> Key findings and analysis of the model:\n",
        "\n",
        "1. Model Architecture:\n",
        "   The model uses a Multi-Layer Perceptron (MLP) neural network implemented with TensorFlow/Keras. It has three dense layers with ReLU activation and dropout layers for regularization.\n",
        "\n",
        "2. Data Preparation:\n",
        "   - The data is loaded from a CSV file containing historical weather data.\n",
        "   - The 'shortwave_radiation' column is used as the target variable.\n",
        "   - The data is normalized using MinMaxScaler to scale values between 0 and 1.\n",
        "   - A sequence length of 10 time steps is used for input features.\n",
        "\n",
        "3. Training Process:\n",
        "   - The model was trained for ten epochs with a batch size 32.\n",
        "   - The training loss decreased slightly from 0.0086 to 0.0080 over the epochs.\n",
        "   - The validation loss fluctuated between 0.0140 and 0.0171, indicating some overfitting.\n",
        "\n",
        "4. Prediction Results:\n",
        "   - The model made predictions on the test set, which were then inverse-transformed to the original scale.\n",
        "   - The predictions were joined with the original data for evaluation.\n",
        "\n",
        "5. Model Performance:\n",
        "   The evaluation metrics show poor performance:\n",
        "   - RMSE (Root Mean Square Error): 215.1385851080044\n",
        "   - MAE (Mean Absolute Error): 142.19445388320852\n",
        "   - R2 (R-squared): -0.3341610588554853\n",
        "   - MAPE (Mean Absolute Percentage Error): 436.75917882994685%\n",
        "\n",
        "6. Interpretation of Results:\n",
        "   - The negative R2 score indicates that the model performs worse than a horizontal line (i.e., predicting the mean of the target variable).\n",
        "   - The high MAPE (436.76%) suggests that the model's predictions are, on average, off by more than four times the actual values.\n",
        "   - The RMSE and MAE values are large, indicating significant prediction errors.\n",
        "\n",
        "7. Data Quality:\n",
        "   - The final dataset has no null or NaN values, which is positive for data quality.\n",
        "   - The 'shortwave_radiation' column (target variable) contains many zero values, which may be challenging for the model to predict accurately.\n",
        "\n",
        "8. Potential Issues:\n",
        "   - The model is overfitting, as indicated by the divergence between training and validation loss.\n",
        "   - The poor performance metrics suggest that the current model architecture or feature set may not be suitable for this prediction task.\n",
        "   - The high number of zero values in the target variable ('shortwave_radiation') may affect the model's ability to learn meaningful patterns.\n",
        "\n",
        "Recommendations:\n",
        "1. Feature engineering: Consider incorporating more relevant features or creating derived features that might better correlate with shortwave radiation.\n",
        "2. Model architecture: Experiment with different neural network architectures, such as LSTM or GRU layers, which might be more suitable for time series data.\n",
        "3. Hyperparameter tuning: Use techniques like grid search or random search to optimize hyperparameters.\n",
        "4. Handle zero values: Investigate the zero values in the target variable and consider strategies to handle them, such as separate models for zero/non-zero prediction.\n",
        "5. Ensemble methods: To compare performance, try ensemble techniques or other machine learning algorithms (e.g., Random Forests, Gradient Boosting).\n",
        "6. Longer training: Increase the number of epochs and implement early stopping to prevent overfitting while allowing for more extended training.\n",
        "7. Data augmentation: Gather more data or use data augmentation techniques to increase the dataset size.\n",
        "\n",
        "In conclusion, while the current model implementation shows poor performance, several avenues exist for improvement. Predicting shortwave radiation based on historical weather data is complex and requires more sophisticated modelling techniques or additional feature engineering to achieve satisfactory results."
      ],
      "metadata": {
        "id": "j-KdD_vXDiOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "RYWsHZg1GSnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def print_df_info(df, name):\n",
        "    print(f\"\\n--- {name} Info ---\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"Sample data:\")\n",
        "    print(df.head())\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Step 1: Load and prepare data\n",
        "        print(\"\\nStep 1: Loading and preparing data...\")\n",
        "        file_path = \"/content/drive/MyDrive/big_data_project/historical_weather.csv\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"The file {file_path} does not exist. Please check the path.\")\n",
        "\n",
        "        # Read only 100k records\n",
        "        df = pd.read_csv(file_path, nrows=100000)\n",
        "        print_df_info(df, \"Original DataFrame\")\n",
        "\n",
        "        # Feature engineering\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "        df['prev_shortwave_radiation'] = df['shortwave_radiation'].shift(1)\n",
        "        df['radiation_change'] = df['shortwave_radiation'] - df['prev_shortwave_radiation']\n",
        "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "        # Select features for prediction\n",
        "        feature_cols = [\"temperature\", \"dewpoint\", \"rain\", \"snowfall\", \"surface_pressure\",\n",
        "                        \"cloudcover_total\", \"windspeed_10m\", \"prev_shortwave_radiation\",\n",
        "                        \"radiation_change\", \"day_of_year\", \"hour\", \"latitude\", \"longitude\"]\n",
        "\n",
        "        # Handle null values\n",
        "        df[feature_cols + [\"shortwave_radiation\"]] = df[feature_cols + [\"shortwave_radiation\"]].fillna(0)\n",
        "\n",
        "        print_df_info(df[feature_cols + [\"shortwave_radiation\"]], \"Processed DataFrame\")\n",
        "\n",
        "        # Prepare data for ML\n",
        "        X = df[feature_cols]\n",
        "        y = df[\"shortwave_radiation\"]\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Step 2: Train Random Forest model\n",
        "        print(\"\\nStep 2: Training Random Forest model...\")\n",
        "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        rf_predictions = rf.predict(X_test)\n",
        "\n",
        "        # Evaluate Random Forest model\n",
        "        rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "        print(f\"Random Forest RMSE: {rf_rmse}\")\n",
        "\n",
        "        # Step 3: Prepare data for neural network\n",
        "        print(\"\\nStep 3: Preparing data for neural network...\")\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "        # Prepare input/output sequences\n",
        "        sequence_length = 24\n",
        "        X_seq, y_seq = [], []\n",
        "        for i in range(len(X_scaled) - sequence_length):\n",
        "            X_seq.append(X_scaled[i:i+sequence_length])\n",
        "            y_seq.append(y_scaled[i+sequence_length])\n",
        "        X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Step 4: Build neural network model\n",
        "        print(\"\\nStep 4: Building neural network model...\")\n",
        "        model = Sequential([\n",
        "            LSTM(64, activation='relu', input_shape=(sequence_length, len(feature_cols))),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "        # Step 5: Train the model with early stopping\n",
        "        print(\"\\nStep 5: Training the model...\")\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "        # Step 6: Make predictions\n",
        "        print(\"\\nStep 6: Making predictions...\")\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        # Step 7: Convert predictions back to original scale\n",
        "        print(\"\\nStep 7: Converting predictions to original scale...\")\n",
        "        predictions_original = scaler.inverse_transform(predictions)\n",
        "        y_test_original = scaler.inverse_transform(y_test)\n",
        "\n",
        "        # Step 8: Evaluate the model\n",
        "        print(\"\\nStep 8: Evaluating the model...\")\n",
        "        mse = mean_squared_error(y_test_original, predictions_original)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_original, predictions_original)\n",
        "        r2 = r2_score(y_test_original, predictions_original)\n",
        "\n",
        "        print(f\"RMSE: {rmse}\")\n",
        "        print(f\"MAE: {mae}\")\n",
        "        print(f\"R2 Score: {r2}\")\n",
        "\n",
        "        # Step 9: Feature importance from Random Forest\n",
        "        print(\"\\nStep 9: Feature importance from Random Forest...\")\n",
        "        feature_importance = rf.feature_importances_\n",
        "        for feature, importance in zip(feature_cols, feature_importance):\n",
        "            print(f\"{feature}: {importance}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6CtJGo0YGTjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This code captures the script's end time, formats timestamps for both start and end times, calculates the total execution time and prints a summary including start time, end time, and duration in a structured format."
      ],
      "metadata": {
        "id": "_Yz1wvIzWgCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "end_time = time.time()\n",
        "end_time_str = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Convert execution time to hours, minutes, seconds\n",
        "time_delta = timedelta(seconds=int(execution_time))\n",
        "hours, remainder = divmod(time_delta.seconds, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "print(f\"\\nScript execution summary:\")\n",
        "print(f\"Started at..: {start_time_str}\")\n",
        "print(f\"Ended at....: {end_time_str}\")\n",
        "print(f\"Total execution time: {hours} hours, {minutes} minutes, and {seconds} seconds\")\n"
      ],
      "metadata": {
        "id": "XLAxabbZWgw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}